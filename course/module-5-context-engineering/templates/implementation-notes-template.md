# Implementation Notes (Trace): [Название фичи/плана]

**Дата:** [YYYY-MM-DD]
**Спецификация:** [Ссылка на spec файл, например, `../specs/task-filtering.spec.md`]
**Агент/Разработчик:** [Кто реализовывал: имя агента, модель, или имя разработчика]
**Статус:** [Completed / Partial / Failed]

> [!TIP]
> - **Удобство разработчика:** "сверься со списком фич", "прочитай транскрипт" — trace должен быть полезен при возвращении к задаче через неделю
> - **DRY:** не дублируйте то, что уже есть в спецификации или AGENTS.md — ссылайтесь, не копируйте. Противоречия между документами хуже отсутствия документации
> - **Timestamps в именах файлов** (`2025-01-15-task-filtering.md`) помогают агенту определить актуальность документа

---

## Что сделано

Краткое описание результата работы. Конкретика, а не общие слова.

**Пример:**
> - Реализован endpoint `GET /api/tasks` с query parameters `status` и `priority`
> - Добавлена валидация фильтров на уровне handler
> - Написано 8 тестов (unit + integration), покрытие 92%
> - Создан составной индекс `(status, priority)` в БД
> - Все acceptance criteria из спецификации выполнены

**Ваш текст:**

[Опишите, что конкретно сделано. Список файлов, функций, тестов, изменений в БД.]

---

## Что сделано хорошо

Что получилось особенно удачно. Конкретика, а не "всё супер".

**Пример:**
> - Query parameter parsing через `url.Values` работает корректно и читаемо
> - Валидация вынесена в отдельную функцию `validateTaskFilters()` — переиспользуется
> - Тесты покрывают все edge cases из спецификации (неверные значения, пустые фильтры)
> - Индекс `(status, priority)` ускорил запрос с 120ms до 15ms на 10K задач

**Ваш текст:**

[Что получилось хорошо? Какие решения были удачными?]

---

## Что не так / Что улучшить

Проблемы, которые возникли. Не просто "была ошибка", а конкретная проблема с причиной и решением.

**Пример:**
> - **Проблема:** Первоначально забыл нормализовать query parameters к lowercase, тесты падали на `?Status=pending`
>   - **Причина:** Спецификация не уточняла регистр
>   - **Решение:** Добавил `strings.ToLower()` в parsing, обновил тесты
>
> - **Проблема:** Integration тесты медленные (~500ms каждый) из-за реальных DB запросов
>   - **Причина:** Не использовал testcontainers, запросы идут в shared DB
>   - **Решение:** Пока оставил как есть, но стоит рассмотреть testcontainers для изоляции
>
> - **Улучшение:** Можно было бы кэшировать результаты фильтрации в Redis для популярных запросов
>   - **Статус:** Out of scope для этой задачи, добавить в backlog

**Ваш текст:**

[Какие проблемы возникли? Что было сложно? Что можно улучшить в будущем?]

---

## Технические решения

Почему выбрали именно такую реализацию. Обоснование для будущих агентов/разработчиков.

**Пример:**
> - **Решение:** Query parameters вместо POST body
>   - **Почему:** REST convention для фильтрации — GET с query params, POST для создания ресурсов
>   - **Альтернатива:** POST /api/tasks/filter с JSON body — семантически неверно для фильтрации
>
> - **Решение:** Валидация на уровне handler, а не middleware
>   - **Почему:** Валидация специфична для этого endpoint, нет смысла выносить в global middleware
>   - **Альтернатива:** Middleware — усложняет без выгоды
>
> - **Решение:** Составной индекс (status, priority) вместо двух отдельных
>   - **Почему:** Большинство запросов фильтруют по обоим параметрам одновременно
>   - **Альтернатива:** Два отдельных индекса — менее эффективны для комбинированных запросов
>
> - **Решение:** OR-логика для множественных значений одного параметра (напр., ?status=pending&status=done)
>   - **Почему:** Спецификация не уточняла, но OR-логика более гибкая для пользователя
>   - **Альтернатива:** Возвращать 400 — слишком строго

**Ваш текст:**

[Какие технические решения приняты? Почему выбрали A вместо B? Какие альтернативы рассматривали?]

---

## Для конституции

Что стоит добавить в `AGENTS.md` (или другой файл конституции) для будущих агентов.

**Пример:**
> - **Pattern:** Фильтрация в REST API через query parameters, не через POST body
> - **Convention:** Query parameters нормализовать к lowercase перед валидацией
> - **Antipattern:** Не использовать POST для фильтрации (нарушает REST semantics)
> - **ADR:** Составные индексы для часто комбинируемых фильтров (статус + приоритет)

**Ваш текст:**

[Что стоит зафиксировать в конституции? Какие паттерны/antipatterns/решения важны для будущих задач?]

---

## Отклонения от спецификации

Если реализация отличается от спецификации — обоснуйте почему.

**Пример:**
> - **Отклонение:** Спецификация не описывала поведение при множественных значениях одного параметра (`?status=pending&status=done`). Реализовал OR-логику вместо возврата ошибки.
>   - **Обоснование:** OR-логика более гибкая и ожидаемая с точки зрения UX
>   - **Требуется согласование:** Да, обновить спецификацию
>
> - **Отклонение:** Не требуется (реализация полностью соответствует спецификации)

**Ваш текст:**

[Есть ли отклонения от спецификации? Если да — почему и требуется ли обновление спецификации?]

---

## Метрики

Количественные метрики по реализации.

**Код:**
- **Файлов изменено/создано:** [число]
- **Строк добавлено:** [число]
- **Строк удалено:** [число]

**Тесты:**
- **Количество тестов:** [число unit + число integration]
- **Coverage:** [процент, например, 92%]
- **Время выполнения тестов:** [например, 3.2s]

**Performance:**
- **Время выполнения запроса:** [например, "15ms на 10K задач"]
- **Использование памяти:** [если измеряли]
- **Нагрузочное тестирование:** [если проводили]

**Пример:**
> - Файлов создано: 2 (handler_test.go, migration 003_add_task_filters_index.sql)
> - Файлов изменено: 3 (handler.go, service.go, repository.go)
> - Строк добавлено: 245
> - Строк удалено: 12
> - Тестов: 5 unit + 3 integration
> - Coverage: 92%
> - Время тестов: 2.1s
> - Запрос: ~15ms на 10K задач (с индексом), было 120ms (без индекса)

**Ваш текст:**

[Заполните метрики, если они релевантны]

---

## Время выполнения

**Планировалось:** [время из спецификации, если указано]

**Фактически затрачено:** [время]

**Разница:** [больше/меньше плана и почему]

**Пример:**
> - Планировалось: 3-5 часов
> - Фактически: 4 часа
> - Разница: в рамках плана. Дополнительное время ушло на отладку индекса и написание edge case тестов.

**Ваш текст:**

[Сколько времени заняла реализация? Укладывались ли в план?]

---

## Pull Request

**PR:** [Ссылка на pull request]

**Статус:** [Open / Merged / Closed]

**Reviewer:** [Кто ревьюил]

**Comments:** [Ключевые комментарии из ревью]

**Пример:**
> - PR: https://github.com/user/project/pull/123
> - Статус: Merged
> - Reviewer: @username
> - Comments: "Отличная работа с тестами, но стоит добавить JSDoc комментарии к validateTaskFilters()"

**Ваш текст:**

[Информация о PR, если он создан]

---

## Зависимости и блокеры

**Зависимости, которые ожидали:**
- [Что ожидалось]

**Зависимости, которые обнаружили:**
- [Что обнаружили в процессе работы]

**Блокеры:**
- [Что блокировало работу]

**Пример:**
> - Ожидали: Миграция для полей status и priority (уже была)
> - Обнаружили: Нужен составной индекс для производительности (добавили)
> - Блокеров не было

**Ваш текст:**

[Зависимости и блокеры, если были]

---

## Lessons Learned

Что узнал агент/разработчик в процессе работы. Полезно для будущих задач.

**Пример:**
> - **Урок 1:** Query parameter parsing в Go требует ручной валидации — url.Values не проверяет типы
> - **Урок 2:** Составные индексы значительно ускоряют фильтрацию, стоит добавлять сразу
> - **Урок 3:** Тесты для edge cases (пустые фильтры, неверные значения) выявляют bugs раньше production
> - **Урок 4:** Спецификация должна явно описывать edge cases, иначе агент додумывает сам

**Ваш текст:**

[Что нового узнали? Какие insights получили?]

---

## Следующие шаги

Что нужно сделать после этой задачи.

**Immediate (срочно):**
- [ ] [Задача 1: например, "Обновить спецификацию с учётом OR-логики"]
- [ ] [Задача 2: например, "Добавить JSDoc комментарии по feedback из ревью"]

**Future (в будущем):**
- [ ] [Задача 1: например, "Рассмотреть кэширование популярных фильтров в Redis"]
- [ ] [Задача 2: например, "Добавить пагинацию для больших списков"]
- [ ] [Задача 3: например, "Реализовать сортировку задач"]

**Пример:**
> **Immediate:**
> - [x] Обновить спецификацию с учётом OR-логики для множественных фильтров
> - [x] Добавить JSDoc комментарии к validateTaskFilters()
>
> **Future:**
> - [ ] Добавить Redis кэш для популярных фильтров (когда нагрузка вырастет)
> - [ ] Рассмотреть testcontainers для изоляции integration тестов

**Ваш текст:**

[Что нужно сделать дальше?]

---

**Конец trace файла.**

> **Примечание для агентов:** Этот trace должен быть создан **после** завершения реализации. Заполняй инкрементально по ходу работы — не пытайся вспомнить всё в конце. Trace — это field notes, а не отчёт постфактум. Список артефактов (созданных/изменённых файлов) доступен через `git diff` — дублировать его в trace не нужно.
