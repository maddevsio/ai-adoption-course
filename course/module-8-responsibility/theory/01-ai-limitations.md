[← Оглавление](../../../README.md)

# Модуль 8: Ответственное использование ИИ

## 1. Ограничения ИИ

#### **Галлюцинации**
Модель может уверенно выдать несуществующую информацию. Ответы LLM убедительны, выглядят правдоподобно. 

Типичные примеры: предложить использовать библиотеку `fast-json-validator` для Node.js с детальным описанием API и ссылкой на npm, хотя такой библиотеки не существует.

Причина: модель считает вероятности, а не проверяет факты. 

Решение: перед имплементации нужно валидировать план через других агентов, проверять особо важные решения самостоятельно.

**Контекстное окно** ограничено. Модель не всегда может поместить все нужные данные в свою память. Она "забывает" начало длинной сессии и начинает бредить.

Решение: систематически сохранять и обслуживать артефакты (см. Модуль 5)

#### **Sycophancy** 
Склонность соглашаться с пользователем. Побочный эффект RLHF — модель оптимизирована быть «полезной», что означает избегание конфликта.

Пример: вы предлагаете использовать `eval()` в JavaScript для парсинга JSON из ненадёжного источника. Модель может написать: "Хорошо, используем eval(). Убедитесь, что данные доверенные."

#### **Предвзятость данных**
Модели обучены на данных из интернета, который полон некачественных данных: устаревший код, субъективные мнения. Модель не знает, какие практики актуальны для вас, если это не было явно в промпте и документации.

Предвзятость проявляется и в выборе технологий: модель может чаще предлагать мейнстримные решения, которые не всегда подходят для задачи. 

Решение: явно указывайте требования, проверяйте актуальность предложенных решений.

#### **Garbage in — garbage out** 
Нечёткое ТЗ → нечёткий код.

Пример из практики: "добавь валидацию" → модель добавит проверку `if (!data) return error`, которая бесполезна. 

Решение: управляйте контекстом проекта, документируйте предпочтения (см. Модуль 5).
---

[← Чеклист готовности](../../module-7-orchestration/practice/06-checklist.md) | [Оглавление](../../../README.md) | [2. Безопасность →](02-security.md)
