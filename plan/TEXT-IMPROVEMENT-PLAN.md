# План улучшения текстов курса: меньше AI-slop, больше читаемости

## Проблема

Материалы курса (~7900 строк, 90 файлов) написаны с участием AI-агентов. Текст работающий и структурированный, но содержит характерные паттерны "AI-slop" — узнаваемый стиль текста, сгенерированного языковой моделью. Это снижает доверие читателя и воспринимается как "ещё один AI-курс про AI".

---

## Часть 1: Критерии AI-slop текста

### 1.1. Лексические маркеры (отдельные слова и фразы)

| # | Маркер | Пример из курса | Почему это slop |
|---|--------|----------------|-----------------|
| L1 | **Усилители без доказательства** — "мощный", "критически важный", "ключевой", "радикально" | "ИИ — **мощный** инструмент" (module-8, line 7); "Контекст **критичен**" (module-4, line 39); "ИИ **радикально** снижает стоимость" (module-4, line 47) | LLM добавляет эмоциональные слова по привычке. Если не подкреплено числом или сравнением — пустое слово |
| L2 | **Hedge-фразы** — "это не просто X, это Y", "не только X, но и Y" | "Это не просто план, а **исполняемый документ**" (module-7, line 9); "Это не просто документация для людей — это **память агента**" (module-5, line 9) | Шаблон для создания ложной глубины. LLM использует его для добавления пафоса |
| L3 | **Квази-метафоры** — "археологические раскопки", "уйти не туда", "забивать гвозди микроскопом" | "каждая новая сессия превращается в **археологические раскопки**" (module-5, line 7) | LLM генерирует метафоры для "оживления" текста, но они звучат шаблонно |
| L4 | **Пустые связки** — "Давайте рассмотрим", "Стоит отметить", "Важно понимать", "Необходимо учитывать" | Встречаются как вводные фразы перед содержательными абзацами | Занимают место, не несут смысла. Читатель и так читает — незачем приглашать его рассмотреть |
| L5 | **Универсальные прилагательные** — "эффективный", "оптимальный", "значительный", "существенный" | "Эффективная работа с агентом начинается с правильной декомпозиции" (module-4, line 31) | LLM ставит эти слова для увеличения "значимости" предложения. Без конкретики они бессмысленны |
| L6 | **Парные перечисления с "и"** — "процесс и результат", "качество и производительность" | Встречаются в описаниях преимуществ | LLM балансирует предложения, добавляя пары. Часто одно из слов избыточно |

### 1.2. Структурные паттерны

| # | Паттерн | Пример из курса | Почему это slop |
|---|---------|----------------|-----------------|
| S1 | **Шаблонная трёхчастность** — каждый раздел идёт по схеме "определение → пример → вывод" без вариаций | Уровни в module-1: каждый ровно из "Что это → Характеристики → Типичные проблемы → Путь на следующий уровень → Пример разработчика" | Ригидная структура убивает сканирование. Читатель перестаёт обращать внимание после 2-го повтора |
| S2 | **Копипаста переходных блоков** — идентичные блоки текста повторяются без адаптации | Блок "Переход на следующий уровень" в module-1 скопирован 4 раза слово-в-слово (lines 36-43, 80-87, 124-133, 168-177) | LLM заполняет шаблон. 4 копии одного текста — явный признак генерации |
| S3 | **Избыточные определения** — каждый термин получает развёрнутое определение при каждом упоминании | "MCP (Model Context Protocol — стандарт подключения внешних инструментов к агенту, подробнее в Модуле 6)" (module-1, line 143); "Constitution — набор правил и принципов работы агента" (module-1, line 121) | Объяснять аббревиатуру стоит один раз. Повторные определения — перестраховка LLM |
| S4 | **Перечисления ради длины** — списки с 5+ пунктами, где 2-3 пункта несут основную мысль, а остальные — парафразы | Таблица навыков в module-1 (lines 224-231): 6 навыков × 5 уровней = 30 ячеек, многие повторяют идею | LLM любит длинные списки. Они имитируют полноту, но размывают главное |
| S5 | **Фиктивные персонажи** — Алексей, Айтмырза, Айгуль, Елена, Антон | По одному на каждый уровень в module-1 | LLM генерирует "персонажей" для "оживления" текста. Без реальной истории это бессмысленная декорация |
| S6 | **Параллельная структура определений** — у каждого понятия ровно 4 подраздела с одинаковыми заголовками | "Инструменты → Промптинг → Надзор → Автономность" повторяется 5 раз в module-1 | Ригидный шаблон. Читатель может отличить уровни 3, 4, 5 только перечитывая, потому что структура идентична |

### 1.3. Семантические паттерны

| # | Паттерн | Пример | Почему это slop |
|---|---------|--------|-----------------|
| M1 | **Тривиальные утверждения в роли инсайтов** — очевидное подаётся как открытие | "Агент не знает структуру проекта... нужно явно указать" (module-4, line 39); "Качество результата прямо зависит от качества вводных данных" (module-8, line 35) | LLM заполняет пространство банальностями. Читатель-разработчик знает, что garbage in = garbage out |
| M2 | **Ложная дихотомия / контраст** — создание оппозиции там, где её нет | "Это не консультант — это исполнитель" (module-4, line 21) | LLM создаёт контрасты для драматизации. Часто оба варианта не исключают друг друга |
| M3 | **Переформулировка вместо нового знания** — одна мысль сказана дважды разными словами | "Агенты не имеют памяти между сессиями. Каждая новая сессия начинается с чистого листа." (module-4, lines 84-84) | LLM добавляет парафразы для увеличения длины. Обе фразы говорят одно и то же |
| M4 | **Преждевременная аналогия** — объяснение через аналогию того, что уже понятно | "Полезная аналогия: чат — это Stack Overflow с интерактивным интерфейсом, агент — это джуниор-разработчик" (module-4, line 27) | Аналогия нужна для непонятного. Для понятия "чат vs агент" она избыточна — читатель уже понял из примера выше |

### 1.4. Формальные маркеры

| # | Маркер | Пример | Описание |
|---|--------|--------|----------|
| F1 | **Жирный текст на каждом шагу** | Module-1: **Инструменты:**, **Промптинг:**, **Надзор:**, **Автономность:** — всё жирное | LLM выделяет жирным слишком щедро. Когда всё важно — ничто не важно |
| F2 | **Parenthetical пояснения** — скобки внутри скобок | "FastAPI dependency injection" в скобках внутри уже скобочного пояснения (module-4, line 41) | LLM дополняет мысль, не умея решить, включить ли деталь или убрать |
| F3 | **Длинные предложения с вложенными оборотами** | Предложение на line 216 в module-1 — 4 строки с 3 вложенными деталями | LLM генерирует поток мысли без рефакторинга на короткие предложения |

---

## Часть 2: Метрики качества текста (измеримые)

### 2.1. Количественные метрики

| # | Метрика | Формула / как считать | Текущее значение (оценка) | Целевое значение |
|---|---------|----------------------|--------------------------|------------------|
| Q1 | **Плотность слов-усилителей** | Кол-во усилителей (мощный, критичный, ключевой, радикальный, значительный, существенный, эффективный, оптимальный) / общее кол-во слов × 100 | ~1.5-2% | < 0.5% |
| Q2 | **Индекс дублирования** | Кол-во дословно повторяющихся блоков (>3 строк) между файлами | ~12 блоков (только "Переход на следующий уровень" повторён 4 раза) | 0 (каждый блок уникален или вынесен в общий файл) |
| Q3 | **Средняя длина предложения** | Кол-во слов / кол-во предложений | ~22-25 слов | 14-18 слов |
| Q4 | **Плотность скобок** | Кол-во скобочных пояснений / кол-во абзацев | ~0.8 (почти в каждом абзаце скобки) | < 0.3 |
| Q5 | **Доля пустых вводных** | Кол-во предложений, начинающихся с "Важно понимать", "Стоит отметить", "Необходимо учитывать" и т.п. / общее кол-во предложений | ~3-5% | < 1% |
| Q6 | **Коэффициент сжимаемости** | (Исходный объём — объём после удаления всех маркеров из 1.1-1.4) / исходный объём | ~15-25% текста можно убрать без потери смысла | < 5% |
| Q7 | **Доля жирного текста** | Кол-во символов в `**...**` / общее кол-во символов | ~8-12% | 3-5% |

### 2.2. Качественные метрики (оценка ревьюером)

| # | Метрика | Шкала | Как оценивать |
|---|---------|-------|---------------|
| R1 | **"Угадай автора"** | 1 (точно AI) — 5 (точно человек) | 3 независимых ревьюера читают случайный отрывок (1 страница) и оценивают, AI или человек написал |
| R2 | **Удержание внимания** | 1-5 | Ревьюер отмечает, на каком абзаце потерял интерес или начал скиммить |
| R3 | **Плотность инсайтов** | количество на страницу | Ревьюер отмечает места, где узнал что-то новое или неочевидное. Целевое: ≥2 инсайта на страницу |
| R4 | **Естественность языка** | 1-5 | Читается как рабочая документация (5) или как маркетинговый лендинг (1)? |

### 2.3. Автоматизированный скрипт проверки

Создать скрипт `scripts/slop-check.sh` (или Python), который проверяет:

```
Входные данные: markdown-файл
Выходные данные: отчёт с метриками Q1-Q7

Проверки:
1. grep на список слов-усилителей → Q1
2. Подсчёт дубликатов блоков (3+ строк) между файлами → Q2
3. Средняя длина предложения (split по . ! ? ) → Q3
4. Подсчёт скобок → Q4
5. grep на список пустых вводных → Q5
6. Подсчёт bold-маркеров → Q7
7. Подсчёт "не просто X, это Y" паттернов
8. Поиск дословно повторяющихся блоков длиннее 3 строк
```

---

## Часть 3: План редактирования

### Фаза 0: Инструментарий (1 день)

- [ ] Написать `scripts/slop-check.py` — автоматический анализатор текста по метрикам Q1-Q7
- [ ] Прогнать на всех 90 файлах, получить baseline метрик
- [ ] Отсортировать файлы по "уровню slop" (суммарный score) — начинать с худших

### Фаза 1: Механическая чистка (2-3 дня)

Применить ко всем 90 файлам без изменения смысла:

| Действие | Что делаем | Пример |
|----------|-----------|--------|
| Убрать слова-усилители | Удалить "мощный", "критически", "ключевой", "радикально" без замены | "ИИ — мощный инструмент, но..." → "ИИ полезен, но..." |
| Убрать пустые вводные | Удалить "Важно понимать", "Стоит отметить", начинать с сути | "Важно понимать, что агент не имеет памяти" → "Агент не имеет памяти" |
| Убрать парафразы | Оставить одну формулировку из двух | "не имеет памяти между сессиями. Каждый раз начинает с нуля" → "не имеет памяти между сессиями" |
| Сократить скобочные пояснения | Вынести в отдельное предложение или удалить | "MCP (Model Context Protocol — стандарт подключения...)" → "MCP — стандарт подключения..." (определение дать один раз в глоссарии) |
| Убрать дублированные блоки | Заменить 4 копии "Переход на следующий уровень" на 1 общий блок в конце файла | — |
| Сократить bold-разметку | Оставить жирный только для терминов при первом определении и для противопоставлений | — |

### Фаза 2: Структурная правка (3-5 дней)

По файлам, отсортированным по размеру (начать с самых длинных):

#### module-1 (01-theory.md, 239 строк → цель: 150-170 строк)
- Заменить 5 одинаковых блоков "Характеристики" (Инструменты/Промптинг/Надзор/Автономность) на сравнительную таблицу — одну на весь файл
- Удалить 4 копии блока "Переход на следующий уровень" — оставить один общий блок
- Персонажи (Алексей, Айтмырза, Айгуль, Елена, Антон): сократить до 1-2 предложений каждый или убрать, заменив на конкретный сценарий без имени
- Объединить таблицу навыков с описаниями уровней вместо двух отдельных представлений

#### module-4 (01-agent-fundamentals.md, 114 строк → цель: 85-95 строк)
- Убрать аналогию "чат = Stack Overflow, агент = джуниор" — читатель уже понял из примера кода
- Сократить секцию "Steering" — 4 шага примера занимают много места, достаточно 2 шагов + ссылки на полный пример
- Раздел "AGENTS.md" — убрать повторное объяснение концепции (уже было в module-1)

#### module-5 (01-context-and-sdd.md, 85 строк → цель: 65-70 строк)
- Убрать "археологические раскопки" метафору
- Сократить пример спецификации — оставить 60% (убрать очевидные поля)
- Убрать "Minimum Viable SDD" — это полезный контент, но сократить пояснения

#### module-7 (04-playbook-and-operations.md, 201 строка → цель: 150-160 строк)
- ASCII-арт Maestro: оставить, но сократить пояснения вокруг
- Секция "Честно про сложности" — хорошая, но убрать слова-усилители
- Секции 7.2-7.5 (Auto Run, worktrees, Group Chat, Wizard) — сократить описания, оставить суть + пример

#### dos-and-donts.md (265 строк → цель: 200-220 строк)
- Убрать дублирование с модулями — оставить только чек-лист без повторных пояснений
- Метафоры типа "Ferrari для поездки в магазин" — оставить, это работающие и короткие
- Объединить секции 8+9 (безопасность + конфиденциальность) — они пересекаются

### Фаза 3: Тональная правка (2-3 дня)

Принципы:
1. **Писать как для коллеги**, а не как для "обучаемого". Тон рабочей документации, не лекции
2. **Убрать менторский тон**: "Вы должны понимать" → просто объяснить
3. **Добавить сухие факты** вместо эмоциональных описаний: "модель забывает контекст" вместо "контекст размывается и агент начинает предлагать решения, не связанные с исходной задачей"
4. **Укоротить примеры персонажей** или убрать имена: вместо "Елена, senior backend-разработчик на Go: Работает через Claude Code..." → "Сценарий: backend на Go, Claude Code + MCP..."

### Фаза 4: Валидация (1 день)

- [ ] Прогнать `scripts/slop-check.py` повторно — сравнить с baseline
- [ ] Провести "Угадай автора" тест с 3 ревьюерами
- [ ] Проверить, что ни один технический факт не потерян (diff review)
- [ ] Проверить все внутренние ссылки (навигация между модулями)

---

## Часть 4: Конкретные примеры редактирования

### Пример 1: module-5, theory/01-context-and-sdd.md, строки 7-9

**До:**
> AI-агент не имеет памяти между сессиями. Каждый раз агент начинает с нуля. Всё, что не помещается в контекстное окно, для агента не существует. Без документированных решений каждая новая сессия превращается в археологические раскопки: агент читает код, пытается угадать архитектуру, делает неверные предположения и переизобретает уже принятые решения.
>
> Решение проблемы — **артефакты**: файлы, которые хранят контекст между сессиями. Это не просто документация для людей — это память агента, его справочник, его инструкция.

**Проблемы:** парафраза (первые два предложения = одно и то же), метафора "археологические раскопки", шаблон "не просто X — это Y", тройное перечисление ("память, справочник, инструкция")

**После:**
> Агент не имеет памяти между сессиями. Без документированных решений он пере-делает уже принятые решения: выбирает не тот логгер, кладёт миграции не туда, использует конкретные типы вместо интерфейсов (реальный пример из Enji Fleet — см. module-5/theory/03).
>
> **Артефакты** — файлы, которые хранят контекст между сессиями и загружаются агенту в начале работы.

**Результат:** 65 слов → 42 слова (−35%). Убрана метафора, парафраза, шаблон. Добавлена ссылка на реальный пример.

### Пример 2: module-4, theory/01-agent-fundamentals.md, строки 6-7

**До:**
> Главное различие между чатом и агентом не в модели, которая лежит в основе, а в способе взаимодействия и уровне автономности.

**Проблема:** предложение с двойным отрицанием ("не в X, а в Y") и абстрактными понятиями

**После:**
> Чат отвечает на вопросы. Агент выполняет задачи: читает файлы, пишет код, запускает тесты, итерирует.

**Результат:** 19 слов → 14 слов. Конкретнее, сразу показывает разницу вместо описания её.

### Пример 3: module-1, 01-theory.md, блоки "Переход на следующий уровень"

**До (повторено 4 раза):**
> **Объективные критерии:**
> - [ ] Освоены все навыки текущего уровня (оценка 4-5/5)
> - [ ] Можете объяснить коллеге технику из следующего уровня
>
> **Не готовы к переходу, если:**
> - Разброс навыков >2 уровней
> - Не понимаете "почему" работает ваш подход (только "как")
> - Не можете воспроизвести результат осознанно

**После (один блок в конце файла):**
> ## Когда переходить на следующий уровень
>
> Три признака готовности (работают для любого уровня):
> 1. Навыки текущего уровня — 4-5/5 по самооценке
> 2. Можете объяснить коллеге технику следующего уровня
> 3. Разброс навыков не более 2 уровней

**Результат:** 4 × 7 строк = 28 строк → 5 строк (−82%).

---

## Часть 5: Приоритеты файлов

Отсортировано по ожидаемому эффекту (больший файл × больше slop = выше приоритет):

| Приоритет | Файл | Строк | Основные проблемы |
|-----------|-------|-------|-------------------|
| 1 | `module-1/01-theory.md` | 239 | S1, S2, S5, S6 — шаблонность, копипаста, фиктивные персонажи |
| 2 | `dos-and-donts.md` | 265 | S4 — длинные списки с пересечениями между секциями |
| 3 | `module-7/theory/04-playbook-and-operations.md` | 201 | L1, L2 — усилители, "не просто X" |
| 4 | `module-5/theory/03-plans-traces-fails.md` | 188 | S3 — избыточные определения при каждом упоминании |
| 5 | `module-4/theory/03-testing-with-agents.md` | 187 | M1, M3 — тривиальные утверждения, парафразы |
| 6 | `module-2/theory/02-available-tools.md` | 181 | L5 — универсальные прилагательные |
| 7 | `self-assessment.md` | 257 | S4 — перечисления ради длины |
| 8 | `module-3/theory/02-advanced-techniques.md` | 160 | Хороший файл — минимальный slop, примеры конкретные |
| 9 | `module-0-intro/theory.md` | 95 | L1, M1 — усилители, банальности |
| 10 | Все остальные файлы (<150 строк) | — | Точечные правки по Q1-Q7 |

---

## Часть 6: Ожидаемый результат

| Метрика | До | После |
|---------|-----|-------|
| Общий объём (строк) | ~7900 | ~5500-6300 (−20-30%) |
| Слова-усилители (Q1) | ~1.5-2% | < 0.5% |
| Дубликаты блоков (Q2) | ~12 | 0 |
| Средняя длина предложения (Q3) | ~22-25 слов | 14-18 слов |
| "Угадай автора" (R1) | ~2/5 | ≥ 3.5/5 |
| Плотность инсайтов (R3) | ~1/стр | ≥ 2/стр |

---

## Ограничения плана

1. **Не трогать техническую суть** — все факты, примеры кода, команды CLI, ссылки остаются
2. **Не трогать структуру модулей** — 8 модулей, порядок, навигация без изменений
3. **Не трогать шаблоны** (templates/) — они предназначены для копирования, там slop допустим
4. **Не трогать диаграммы** — они и так короткие и конкретные
5. **Язык** — русский, сохраняем "ты" для инструкций
6. **Не добавлять новый контент** — только убирать и перефразировать существующий
