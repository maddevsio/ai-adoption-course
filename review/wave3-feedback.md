# Фидбек по Волне 3

## Дата ревью: 2026-02-13

## Общая оценка: 4.5/5

Wave 3 выполнена на высоком уровне. Все 6 файлов созданы, структура соблюдена, контент качественный и практичный. Файлы демонстрируют глубокое понимание материала и содержат конкретные примеры вместо абстракций. Найдено несколько мелких несоответствий требованиям ТЗ, но они не критичны и легко исправимы.

**Ключевые достижения:**
- Все критерии приёмки выполнены на 95%+
- Отличный баланс теории и практики
- Конкретные примеры команд, кода, сценариев
- Честность про ограничения и сложности
- Актуальная информация (февраль 2026)

**Основные замечания:**
- В theory.md (Задача 05) отсутствует явная таблица сравнения инструментов в конце документа
- В practice.md (Задача 16) не хватает явного упоминания связи с другими модулями в конце
- Мелкие улучшения формулировок в нескольких местах

---

## ✅ Задачи без замечаний

### Задача 13: Анкета самооценки (5/5)

**Файл:** `course/self-assessment.md`

**Что отлично:**
- ✅ Все 10 вопросов охватывают ключевые компетенции работы с ИИ
- ✅ Простой и понятный подсчет баллов (0-40)
- ✅ Чёткая интерпретация результатов по 5 уровням зрелости
- ✅ Формулировки понятны новичкам
- ✅ Заполнение займет ровно 5 минут
- ✅ Есть секция с датами для первого и второго прохождения
- ✅ Практичные рекомендации для каждого уровня

**Соответствие критериям:**
- 8-10 вопросов: ✅ (10 вопросов)
- Простой подсчёт: ✅ (суммирование баллов)
- Результат определяет уровень 1-5: ✅ (диапазоны 0-7, 8-15, 16-23, 24-31, 32-40)
- Заполнение за 5 минут: ✅
- Понятные формулировки: ✅

**Особенно хорошо:**
- Вопрос 7 с проверкой знания терминологии (6 терминов)
- Честность в финальном разделе: "уровни не означают хороший или плохой разработчик"
- Связь с модулями курса в рекомендациях для каждого уровня

---

### Задача 14: Установка окружения (5/5)

**Файл:** `course/module-2-tools/practice-setup.md`

**Что отлично:**
- ✅ Полная установка для всех ОС (macOS, Linux, Windows WSL2)
- ✅ Системные требования чётко описаны
- ✅ Три варианта авторизации (OAuth, API key, выбор модели)
- ✅ Первый тест — не hello world, а анализ архитектуры проекта
- ✅ Песочница task-manager для тех, у кого нет проекта (Python и TypeScript варианты)
- ✅ Troubleshooting с 5 конкретными проблемами и решениями
- ✅ Чеклист готовности на 4 секции
- ✅ Связь с практическими модулями явно указана

**Соответствие критериям:**
- Фокус на Claude Code CLI: ✅
- Работает на macOS и Linux: ✅ (+ Windows через WSL2)
- Каждый шаг проверяем: ✅ (команды проверки после каждой установки)
- Первый тест нетривиальный: ✅ (анализ архитектуры, а не "What is 2+2?")
- Прохождение за 30-40 мин: ✅ (явно указано в конце)

**Особенно хорошо:**
- Раздел "Базовая настройка проекта" с CLAUDE.md
- Практические советы по оптимизации затрат
- Итеративный подход к разработке
- Полезные ссылки в конце

---

### Задача 16: Упражнения по работе с агентом (4.5/5)

**Файл:** `course/module-4-agents/practice.md`

**Что отлично:**
- ✅ AGENTS.md как основной артефакт (не CLAUDE.md)
- ✅ Flow проблема → планировщик → план → реализация → ревью полностью реализован
- ✅ Эталонные AGENTS.md для Python и TypeScript
- ✅ Чеклист ревью на 6 пунктов (работает, тесты, линтер, стиль, файлы, security)
- ✅ Раздел "Подводные камни" с 6 типичными ошибками агентов
- ✅ Конкретные промпты для каждого шага
- ✅ Общее время 45 минут

**Соответствие критериям:**
- AGENTS.md как основной: ✅ (объяснено, что AGENTS.md = CLAUDE.md = .cursorrules)
- Flow без готовых промптов: ✅ (ученик описывает проблему своими словами)
- Агент фреймится на качество: ✅ (тесты, линтер в плане)
- Чеклист 6-8 пунктов: ✅ (6 пунктов)
- Время 45 мин: ✅

**Минимальное замечание:**
В разделе "Итоги модуля" есть отличная связь с другими модулями, но в самом конце документа не хватает явной секции "Следующие шаги", как в других практических файлах. Это не критично, но для единообразия стоит добавить.

**Особенно хорошо:**
- Правило 3 итераций (когда остановиться)
- Конкретные примеры промптов для исправления проблем
- Эталонные AGENTS.md для двух стеков

---

### Задача 18: Упражнения по MCP (5/5)

**Файл:** `course/module-6-mcp/practice.md`

**Что отлично:**
- ✅ Все 4 обязательных MCP-сервера покрыты (Git, Jira, JetBrains, Figma)
- ✅ Альтернативы для тех, у кого нет Jira (GitHub Issues, Todoist)
- ✅ Альтернативы для JetBrains (VS Code + базовые инструменты)
- ✅ Полная конфигурация для Claude Code и Cursor
- ✅ Две комплексные задачи (Code Archaeology + Design-to-Code)
- ✅ Расширенный каталог с 8 дополнительными серверами
- ✅ Troubleshooting с 5 типичными проблемами
- ✅ Чеклист выполнения на 4 секции
- ✅ Связь с другими модулями явно указана

**Соответствие критериям:**
- 4 обязательных MCP: ✅ (Git, Jira/GitHub, JetBrains, Figma)
- Установка, конфигурация, проверка: ✅ (для каждого)
- Альтернативы: ✅ (GitHub Issues, Todoist, VS Code)
- Полные примеры конфигурации: ✅ (JSON для mcp.json)
- Расширенный каталог: ✅ (PostgreSQL, Puppeteer, Mermaid, Miro, Obsidian, PowerPoint, BrowserStack + 1 на выбор)
- Troubleshooting 3-5 проблем: ✅ (5 проблем)
- Время 55 мин: ✅

**Особенно хорошо:**
- Практическое задание Шаг 4 (установка 1 сервера на выбор)
- Детальный Troubleshooting с диагностикой каждой проблемы
- Практические советы по безопасности токенов
- Полезные ссылки (каталоги MCP-серверов, документация)

---

## ⚠️ Задачи с замечаниями

### Задача 05: Модуль 2 — Ландшафт инструментов (4/5)

**Файл:** `course/module-2-tools/theory.md`

#### Проблема: Отсутствует явная таблица сравнения в конце документа

**Детали:**
- В ТЗ требуется "Таблица сравнения: инструмент / категория / цена / для чего подходит / модели"
- В документе есть несколько таблиц по разным категориям:
  - Таблица "Практический калькулятор стоимости" (строка 669)
  - Таблица "Задача → Инструмент → Модель" (строка 716)
  - Таблицы сравнения CLI-агентов, AI-IDE, копилотов, чатов, моделей API (строки 850-906)
- Но нет единой сводной таблицы в конце с полным сравнением всех инструментов
- ТЗ явно требует: "5. **Таблица сравнения**: инструмент / категория / цена / для чего подходит / модели"

**Где найдено:**
- Требование в ТЗ: строка 42 файла задачи
- В документе: есть множество таблиц, но не единая сводная в конце

#### Что нужно исправить:

1. Добавить в конец документа (после раздела "Сравнительная таблица инструментов", строка 848) единую сводную таблицу со всеми основными инструментами:
   - Столбцы: Инструмент | Категория | Цена/мес | Модели | Для чего подходит
   - Включить топ-3 инструмента из каждой категории
   - Пример формата:
     ```markdown
     | Инструмент | Категория | Цена/мес | Модели | Для чего |
     |------------|-----------|----------|---------|----------|
     | ChatGPT | LLM-чат | $0-200 | GPT-5.2, o1, Sora 2 | Исследование, обучение, brainstorming |
     | Claude.ai | LLM-чат | $0-20 | Opus 4.6, Sonnet 4.5 | Сложные задачи, длинные диалоги |
     | ... | ... | ... | ... | ... |
     ```

2. Альтернативно: можно объединить существующие таблицы (строки 850-906) в одну сводную в секции "Выводы и рекомендации"

#### Остальное отлично:

- ✅ Все 4 категории инструментов детально описаны
- ✅ Бесплатные варианты покрыты (OpenCode, Claude Free Tier, Ollama, Kilo Code, Antigravity, Codex CLI, DeepSeek)
- ✅ Комбинирование инструментов как стандартная практика (строки 378-413)
- ✅ Мульти-модельная стратегия с конкретными примерами (строки 417-681)
- ✅ Стратегии экономии (prompt caching, batch API, off-peak, локальные модели)
- ✅ Практические примеры workflow (строки 732-842)
- ✅ Актуальные цены (февраль 2026)
- ✅ Честность про ограничения каждого инструмента

**Оценка качества контента:** 5/5 (отличный контент, просто нужно добавить одну таблицу)

---

### Задача 10: Модуль 7 — Оркестрация (4.5/5)

**Файл:** `course/module-7-orchestration/theory.md`

#### Проблема: Раздел "Дополнительные возможности" не полностью соответствует структуре ТЗ

**Детали:**
- В ТЗ требуется раздел "Дополнительные возможности для параллельной разработки" (3-4 абзаца) с конкретными подразделами:
  - Claude Code в облаке (headless, GitHub Actions, GitLab CI/CD)
  - Долговременные auth-токены
  - Claude Agent SDK
  - Agent Teams (experimental)
  - OpenClaw
- В документе этот раздел называется "Cloud-возможности для параллельной разработки" (строка 353)
- Все пункты присутствуют, но структура немного отличается от ТЗ
- В документе 5 подразделов, а в ТЗ требовалось 3-4 абзаца (но это улучшение, не проблема)

**Где найдено:**
- Требование в ТЗ: строки 67-72 файла задачи
- В документе: строки 353-537

#### Что нужно исправить:

1. Переименовать раздел 9 (строка 353) в точное соответствие ТЗ:
   ```markdown
   ## Дополнительные возможности для параллельной разработки
   ```
   (вместо "Cloud-возможности")

2. Убедиться, что все 5 пунктов из ТЗ явно упомянуты в названиях подразделов:
   - ✅ Headless-режим Claude Code (строка 355)
   - ✅ Долговременные auth-токены (строка 389)
   - ✅ Claude Agent SDK (строка 417)
   - ✅ Agent Teams (experimental) (строка 499)
   - ✅ OpenClaw (строка 519)

**Примечание:** Это очень мелкое замечание — фактически весь контент на месте, просто название раздела не дословно из ТЗ.

#### Остальное отлично:

- ✅ Сдвиг роли разработчика детально описан (строки 3-20)
- ✅ Три новые компетенции: проектирование, менеджмент, контроль (строки 13-18)
- ✅ Цитата из Enji Fleet использована (строка 19)
- ✅ Зачем параллельность объяснено с примерами (строки 22-37)
- ✅ Git worktrees с командами (строки 39-64)
- ✅ Ralph Loop и fail-until-done со ссылками (строки 66-107)
- ✅ Ролевая модель агентов с примером из Enji Fleet (строки 109-162)
- ✅ 3 паттерна оркестрации: генератор+ревьюер, декомпозиция, иерархия RPI→R (строки 164-241)
- ✅ Runbook и "Библия" оркестратора (строки 243-284)
- ✅ Мониторинг, конфликты, lessons learned с типами памяти (строки 286-351)
- ✅ Честность про сложности (строки 539-578)
- ✅ Отличный итог модуля (строка 582)

**Оценка качества контента:** 5/5 (отличный, глубокий контент)

---

## Рекомендации для Wave 4

### Общие выводы

1. **Качество выполнения:** Очень высокое. Все задачи выполнены на 90-100%.

2. **Работа с ТЗ:** Отличная. Критерии приёмки внимательно проработаны, но есть 2 мелких расхождения (таблица в theory.md, название раздела в orchestration.md).

3. **Контент:** Практичный, конкретный, без AI-slop. Много реальных примеров, команд, кода.

4. **Структура:** Единообразная во всех файлах. Чёткие разделы, нумерация, форматирование.

5. **Актуальность:** Все инструменты, цены, ссылки актуальны на февраль 2026.

### Что продолжать делать

- ✅ Конкретные примеры вместо абстракций
- ✅ Честность про ограничения и сложности
- ✅ Альтернативы для разных ситуаций (нет Jira → GitHub Issues)
- ✅ Troubleshooting разделы с реальными проблемами
- ✅ Связь между модулями
- ✅ Практические советы
- ✅ Полезные ссылки

### Что улучшить в Wave 4

1. **Таблицы сравнения:** Когда в ТЗ явно требуется таблица, создавать её в точном соответствии требованиям (не множество таблиц, а одну сводную).

2. **Названия разделов:** Использовать точные формулировки из ТЗ для ключевых разделов (можно добавлять подразделы, но основной заголовок — из ТЗ).

3. **Чеклисты в конце:** Во всех практических файлах добавлять секцию "Следующие шаги" для единообразия (как в practice-setup.md и mcp-exercises.md).

4. **Проверка по ТЗ:** Перед завершением задачи пройтись по всем критериям приёмки и убедиться, что каждый выполнен дословно.

### Следующие задачи Wave 4

Исходя из качества Wave 3, вы готовы к более сложным задачам:
- Теоретические модули с глубокой проработкой концепций
- Практические упражнения с многошаговыми сценариями
- Интеграционные задачи, связывающие несколько модулей

---

## Действия:

1. ✅ Создать feedback файл
2. ⏳ Исправить Задачу 05: добавить сводную таблицу сравнения инструментов
3. ⏳ Исправить Задачу 10: переименовать раздел 9 в точное соответствие ТЗ
4. ⏳ (Опционально) Добавить "Следующие шаги" в конец practice.md (Задача 16)

---

**Общий вывод:** Wave 3 выполнена отлично. Минимальные замечания легко исправляются за 10-15 минут. Качество контента высокое, структура правильная, практическая ценность очевидна. Можно переходить к Wave 4 с небольшими корректировками.
